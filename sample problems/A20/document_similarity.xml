
<problem>
    <script>
from math import factorial as f
from math import ceil
from math import sqrt
from math import exp
from math import log
from math import erf
from math import e
import random

def C(n,m):
    return f(n)/f(m)/f(n-m)

def myrandom(start,stop,step=1):
    return random.randint(0, int((stop - start) / step)) * step + start

def Phi(x):
    #Cumulative distribution function for the standard normal distribution
    return (1.0 + erf(x / sqrt(2.0))) / 2.0

def Q(x):
	return 1-Phi(x)


n = myrandom(1,100,1)
p = myrandom(0.01,0.99,0.01)

p2 = myrandom(0.87,0.93,0.01)
diff = p2 - 0.8
significance = myrandom(0.01,0.05,0.01)

answer1 = 1.0*p
answer2 = 1.0*p*(1-p)/n
answer3 = 1.0*1/(4*n)
answer4 = 1.0*Q(2*diff*sqrt(n))
answer5 = 1.0*Q(2*diff*sqrt(n))
answer6 = 1.0*Q(2*diff*sqrt(n))
answer7 = 1.0*Q(2*diff*sqrt(n))
answer8 = 1.0*(Qinv(significance)/(2*diff))**2

</script>
<p><br/>
<br/>
You encountered a technique for approximating the Jaccard similarity between documents in problem 7 of Assignment 19.  This technique is called min-hashing.  <br/>
To refresh your memory:<br/>
- We represent documents by the set of vocabulary indices contained in those documents.  If we had an English dictionary indexed by a=0, aah=1, ...the=97124,apple=4337,...zyzzyvas=109581, the sentence "The apple on the tree is red" would be transformed into \{4377, 47647, 50764, 78524, 97124, 99593\} since "the"=97124, "apple"=4337, ...  Notice "the"=97124 is only counted once, even though it appears twice in the sentence.  Henceforth call the set of vocabulary indices \(V\).  <br/>
- The Jaccard similarity between two documents represented by sets \(A \subseteq V\) and \(B \subseteq V\) is \(|A \cap B|/|A \cup B|\).  <br/>
- A minhash \(h\) hashes documents to elements of \(V\).  For documents A,B, \(Pr(h(A) = h(B)) = |A \cap B|/|A \cup B| = J(A,B)\), the Jaccard similarity between A and B.  <br/>
--------------------------------------------------------------------------<br/>
Thus, we can use a number of independent min-hashes \(h_1, ...h_n\) to compute an approximation \(H = \frac {\sum_{1}^{n}{\mathbb{1}\{h_i(A)=h_i(B)\}} }{n}\) of \(J(A,B)\) <br/>
We will bound the accuracy of this sampling approximation to the Jaccard similarity in terms of \(n\).  <br/>
Let \(p\) be the jaccard similarity between some documents A,B. In terms of \(p\) and \(n\):<br/>
<b> \(\mathbb{E}(H) = \)<numericalresponse answer="$answer1">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p><br/>
</b> \(var(H) = \)<numericalresponse answer="$answer2">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p><br/>
What is the smallest upper bound on \(var(H)\) in terms of \(n\)?  Hint: you've computed this result before by taking the derivative with respect to \(p\), setting to \(0\), and solving.  <numericalresponse answer="$answer3">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p><br/>
----------------------------------------------------------------------------<br/>
Assume \(n\) is large enough that \(H\) is approximately normal.  If p &gt; $p2, what is an upper bound on \(P(H &lt; 0.8)\) in terms of the Q function and n?  <numericalresponse answer="$answer4">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p> <br/>
Let's say we define two documents A,B with J(A,B) &gt; $p2 as similar, and those with J(A,B) &lt; 0.8 as dissimilar.  <br/>
Let's say we use the following algorithm to guess whether documents are similar:<br/>
- Compute H using \(n\) independent has functions.  <br/>
- If H &gt; $p2, we guess the documents are similar.  <br/>
- If H &lt; 0.8, we guess the documents are dissimilar.  <br/>
- Else, we output "I don't know"<br/>
---------------------------------------------------------------------------<br/>
If \(0.8 \leq p \leq $p2\), the algorithm is judged correct, no matter what it outputs.  <br/>
Hint: You should be able to quickly answer the following questions using the work above:<br/>
- If p &gt; $p2 for two documents A,B, in terms of \(n\), what is an upper bound on the probability our algorithm incorrectly says these documents are dissimilar?  <numericalresponse answer="$answer5">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p> <br/>
- If p &lt; 0.8, what is an upper bound on the probability our algorithm incorrectly says these documents are similar?  <numericalresponse answer="$answer6">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p> <br/>
What is a simple lower bound on the accuracy of our similarity algorithm, that will hold for any fixed \(p\)?  <numericalresponse answer="$answer7">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p>  <br/>
How many min-hash functions \(n\) are needed (as an expression, don't round) to ensure the probability of an error is &lt; $significance?  You can use Qinv.  <numericalresponse answer="$answer8">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p><br/>
</p>
</problem>