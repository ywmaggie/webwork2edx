
<problem>
    <script>
from math import factorial as f
from math import ceil
from math import sqrt
from math import exp
from math import log
from math import erf
import random

def C(n,m):
    return f(n)/f(m)/f(n-m)

def myrandom(start,stop,step=1):
    return random.randint(0, int((stop - start) / step)) * step + start

def Phi(x):
    #Cumulative distribution function for the standard normal distribution
    return (1.0 + erf(x / sqrt(2.0))) / 2.0

def Q(x):
	return 1-Phi(x)








answer1 = "(n-1)n/2"
answer2 = 1.0*3/(8+11-3)

</script>
<p><br/>

<br/>
Near-duplication is pervasive in the web: there are large numbers of distinct URLs which have exactly<br/>
the same content but differ only in unimportant details like headers and footers. The user of a search<br/>
engine would not be pleased if the answer to his query was a set of 10 near-identical pages! In order<br/>
to remove this redundancy, we need to define a notion of _similarity_ between documents.<br/>
<br/>
For any document---call it \(d\)---let the set of all words in \(d\) be denoted \(C(d)\). For two documents <br/>
\(d\) and \(d'\), we will measure their similarity by the function<br/>
<b><br/>
\( S(d,d') \ = \ \frac{|C(d) \cap C(d')|}{|C(d) \cup C(d')|} .\)<br/>
</b><br/>
If the two documents are truly identical, \(S(d,d') = 1\). If they are almost-identical, \(S(d,d')\) will<br/>
be close to 1. And if they are completely different, with no words in common, then \(S(d,d')\) will be<br/>
zero. We'll consider \(d\) and \(d'\) to be near-duplicates if \(S(d,d')\) is sufficiently close to 1.<br/>
Now, imagine a search engine that is going through a list of documents or webpages, and wants to <br/>
eliminate near-duplicates. Here's an algorithm it could use:<br/>
<b> \({\cal D} = \emptyset\) (set of documents, initially empty)<br/>
</b> for each document \(d\) that appears:<br/>
    - if \(S(d,d')\) is significantly smaller than 1 for all \(d'\) in \({\cal D}\): add \(d\) to \({\cal D}\)<br/>
The final set of documents \({\cal D}\) will contain no near-duplicates. This is good, but the<br/>
algorithm is very slow. Suppose for the sake of simplicity that there are \(n\) documents in total,<br/>
each of length \(L\). Then computing the similarity between two documents takes \(O(L)\) time, and there are <formularesponse type="ci" samples="n@1:100#10" answer="$answer1">
                    <responseparam type="tolerance" default="0.00001"/>
                    <formulaequationinput />
                </formularesponse>
</p><p> pairs, thus the algorithm takes \(O(n^2L)\). This quadratic dependence on \(n\) is prohibitive in web-scale applications,<br/>
where \(n\) could easily be in the billions or tens of billions.<br/>
To get a faster algorithm, we once again resort to hashing.<br/>
---<br/>
Again for the example:<br/>
A: \(\verb|Thanksgiving may be the perfect day to give thanks, but what about giving thanks to a delicious and flawless turkey?|\)<br/>
B: \(\verb|Pick the perfect centerpiece for your Thanksgiving or holiday meal from our ultimate collection of roast turkey recipes.|\)<br/>
After preprocessing, the document A is represented by the <b>set</b>  \(\{\verb|Thanksgiving, perfect, day, give, thanks, delicious, flawless, turkey|\}\), and<br/>
document B becomes the <b>set</b> \(\{\verb|pick, perfect, centerpiece, Thanksgiving, holiday, meal, ultimate, collection, roast, turkey, recipe |\}\)<br/>
The similarity between A and B is <numericalresponse answer="$answer2">
                    <responseparam type="tolerance" default="1%" />
                    <formulaequationinput />
                </numericalresponse>
</p><p><br/>
</p>
</problem>